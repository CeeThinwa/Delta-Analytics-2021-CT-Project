{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "material-animal",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis for the Delta Analytics Teaching Fellowship\n",
    "\n",
    "**Author:** *Cynthia Thinwa*\n",
    "\n",
    "## INTRODUCTION\n",
    "\n",
    "### DATA HANDLING PRACTICES:\n",
    "\n",
    "* Based on Twitter API best practice, the actual data will not be shared, only Twitter's tweet IDs for future reference\n",
    "* The data will be cleaned to remove personally identifiable information like emails and phone numbers\n",
    "* Content published on social media platforms belongs to the public domain with the various authors having given consent\n",
    "\n",
    "Data collection was done from a Microsoft command line running\n",
    "\n",
    "`twint -s %23kot --since \"2020-06-01 03:00:00\" --until \"2021-06-01 03:00:00\" -o kotdata.csv --csv`\n",
    "\n",
    "## EXPLORATORY DATA ANALYSIS\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The raw data was loaded as follows, with the following characteristics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(dplyr)\n",
    "library(wordcloud)\n",
    "library(RColorBrewer)\n",
    "library(rtweet)\n",
    "library(tidytext)\n",
    "library(ggplot2)\n",
    "library(wordcloud2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-sailing",
   "metadata": {},
   "source": [
    "1. The number of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "url <- \"C:/storage/Personal drive backup/Career/Post-Masters/Delta Analytics Teaching Fellowship/EDA/2.kotdata.csv\"\n",
    "DATFdata <- read.delim(url)\n",
    "dim(DATFdata)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-warrant",
   "metadata": {},
   "source": [
    "\n",
    "2. The number of unique conversations had:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "DATFdata$conversation_id <- factor(DATFdata$conversation_id)\n",
    "DATFdata$id <- factor(DATFdata$id)\n",
    "\n",
    "dim(as.data.frame(table(DATFdata$conversation_id)))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-telling",
   "metadata": {},
   "source": [
    "\n",
    "3. The number of unique users speaking:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "DATFdata$user_id <- factor(DATFdata$user_id)\n",
    "\n",
    "dim(as.data.frame(table(DATFdata$user_id)))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-nightmare",
   "metadata": {},
   "source": [
    "\n",
    "4. The most frequent language of posting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "lang <- as.data.frame(table(DATFdata$language))\n",
    "colnames(lang) <- c('Language','Frequency')\n",
    "head(lang[order(lang$Freq, decreasing = TRUE),],n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "DATFdata$tweet[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-spread",
   "metadata": {},
   "source": [
    "\n",
    "5. The date on which most tweets were posted (tweets were from 1st June 2020 UTC+3 upto 1st June 2021 UTC+3): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "dates <- as.data.frame(table(DATFdata$date))\n",
    "colnames(dates) <- c('Date','Frequency')\n",
    "head(dates[order(dates$Freq, decreasing = TRUE),],n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-dylan",
   "metadata": {},
   "source": [
    "\n",
    "### Text transformation\n",
    "\n",
    "Text cleaning was as follows, using `eng_tweets$tweet[4]` as an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Get organic tweets first; found that all tweets were organic!\n",
    "\n",
    "# get only English ones:\n",
    "eng_tweets <- DATFdata[DATFdata$language=='en',]; print(eng_tweets$tweet[4])\n",
    "\n",
    "# Remove funny symbols\n",
    "eng_tweets$tweet <- iconv(eng_tweets$tweet, from = 'UTF-8', to = 'ISO-8859-1', sub = ''); print(eng_tweets$tweet[4])\n",
    "\n",
    "eng_tweets$tweet <- iconv(eng_tweets$tweet, from = 'ISO-8859-1', to = 'UTF-8', sub = ''); print(eng_tweets$tweet[4])\n",
    "\n",
    "eng_tweets$tweet <- gsub(\"https\\\\S*\", \"\", eng_tweets$tweet); print(eng_tweets$tweet[4]) #remove urls\n",
    "\n",
    "eng_tweets$tweet <- gsub(\"@\", \"\", eng_tweets$tweet); print(eng_tweets$tweet[4]) #remove mentions symbol\n",
    "\n",
    "eng_tweets$tweet <- gsub(\"#*\", \"\", eng_tweets$tweet); print(eng_tweets$tweet[4]) #remove hashtags symbol\n",
    "\n",
    "eng_tweets$tweet <- gsub(\"[\\r\\n]\", \" \", eng_tweets$tweet); print(eng_tweets$tweet[4]) #remove newline characters\n",
    "\n",
    "#(we have separate columns with the details)\n",
    "# Punctuation was managed as follows:\n",
    "eng_tweets$tweet <- gsub(\"'\", \"\", eng_tweets$tweet); print(eng_tweets$tweet[4])\n",
    "\n",
    "eng_tweets$tweet <- gsub(\"[[:punct:]]\", \" \", eng_tweets$tweet); print(eng_tweets$tweet[4])\n",
    "\n",
    "eng_tweets$tweet <- gsub(\"amp\", \"\", eng_tweets$tweet); print(eng_tweets$tweet[4]) # remove ampersands\n",
    "\n",
    "# Finally, everything was made lowercase\n",
    "eng_tweets$tweet <- tolower(eng_tweets$tweet); print(eng_tweets$tweet[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Tokenize words\n",
    "Words <- eng_tweets %>%\n",
    "  select(tweet) %>%\n",
    "  unnest_tokens(word, tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-birth",
   "metadata": {},
   "source": [
    "\n",
    "### Word Frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "Words %>% # gives you a bar chart of the most frequent words found in the tweets\n",
    "  count(word, sort = TRUE) %>%\n",
    "  top_n(15) %>%\n",
    "  mutate(word = reorder(word, n)) %>%\n",
    "  ggplot(aes(x = word, y = n)) +\n",
    "  geom_col() +\n",
    "  xlab(NULL) +\n",
    "  coord_flip() +\n",
    "  labs(y = \"Count\",\n",
    "       x = \"Unique words\",\n",
    "       title = \"Most frequent words found in the #KOT tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# remove stop words\n",
    "Words <- Words %>%\n",
    "  anti_join(stop_words)\n",
    "\n",
    "Words %>% # gives you a bar chart of the most frequent words found in the tweets\n",
    "  count(word, sort = TRUE) %>%\n",
    "  top_n(15) %>%\n",
    "  mutate(word = reorder(word, n)) %>%\n",
    "  ggplot(aes(x = word, y = n)) +\n",
    "  geom_col() +\n",
    "  xlab(NULL) +\n",
    "  coord_flip() +\n",
    "  labs(y = \"Count\",\n",
    "       x = \"Unique words\",\n",
    "       title = \"Most frequent words found in the #KOT tweets\",\n",
    "       subtitle = \"Stop words removed from the list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-probability",
   "metadata": {},
   "source": [
    "\n",
    "### Hashtag Frequency\n",
    "\n",
    "All tweets have the hashtag `#KOT`, but we are interested in what else this community talks about, so this hashtag will have to be removed from our visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "eng_tweets$hashtags <- as.character(eng_tweets$hashtags)\n",
    "\n",
    "eng_tweets$hashtags <- gsub(\"[[:punct:]]\", \"\", eng_tweets$hashtags)\n",
    "\n",
    "# every hashtag was made lowercase\n",
    "eng_tweets$hashtags <- tolower(eng_tweets$hashtags)\n",
    "\n",
    "# and #KOT was removed before tokenization\n",
    "eng_tweets$hashtags <- gsub(\"kot\", \"\", eng_tweets$hashtags)\n",
    "\n",
    "Hashtags <- eng_tweets %>%\n",
    "  select(hashtags) %>%\n",
    "  unnest_tokens(word, hashtags)\n",
    "\n",
    "Hashtags_count <- as.data.frame(table(Hashtags$word))\n",
    "\n",
    "viz1 <- wordcloud2(Hashtags_count, size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-hospital",
   "metadata": {},
   "source": [
    "![](viz1.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "head(Hashtags_count[order(Hashtags_count$Freq, decreasing = TRUE),], n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-passage",
   "metadata": {},
   "source": [
    "Based on the results above\n",
    "\n",
    "* platform (`#loyals`),\n",
    "* patriotism (`#kenya` and `#nairobi`),\n",
    "* radio (`#mainaandkingangi` and `#teamclassic`),\n",
    "* political trends (`#bbinonsense` and `#punguzamizigo`)\n",
    "* job ads(`#ikokazike` and `#ikokazi`) and\n",
    "* cats (`#cat` and `#gato`)\n",
    "\n",
    "took centre stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-brunei",
   "metadata": {},
   "source": [
    "\n",
    "### People of Influence\n",
    "\n",
    "In this area, we are interested in identifying people mentioned most frequently.\n",
    "\n",
    "The text, using `eng_tweets$mentions[4]` as an example was first converted as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "eng_tweets$mentions[4]\n",
    "# Remove funny symbols\n",
    "eng_tweets$mentions <- iconv(eng_tweets$mentions, from = 'UTF-8', to = 'UTF-8');eng_tweets$mentions[4]\n",
    "\n",
    "eng_tweets$mentions <- stringr::str_replace_all(\n",
    "  eng_tweets$mentions, \"\\'\", \"\\\"\")\n",
    "eng_tweets$mentions <- gsub(\"[\", \"\", eng_tweets$mentions, fixed = TRUE)\n",
    "eng_tweets$mentions <- gsub(\"]\", \"\", eng_tweets$mentions, fixed = TRUE)\n",
    "\n",
    "df <- as.data.frame(eng_tweets$mentions); dim(df)\n",
    "df[df == \"\"] <- NA  \n",
    "df <- na.omit(df); dim(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get file location\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "write.csv(df$`eng_tweets$mentions`,'C:/Users/CT/Documents/GitHub/Delta-Analytics-2021-CT-Project/dicts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = pd.read_csv('dicts.csv')\n",
    "dicts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the column\n",
    "dicts = dicts.drop(labels='Unnamed: 0', axis=1)\n",
    "dicts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dicts_copy = dicts.copy()\n",
    "single_dicts_copy.iloc[:,0] = single_dicts_copy.iloc[:,0].str.replace('{','',regex=False)\n",
    "single_dicts_copy.iloc[:,0] = single_dicts_copy.iloc[:,0].str.replace('}','',regex=False)\n",
    "single_dicts_copy.iloc[:,0] = single_dicts_copy.iloc[:,0].str.replace(' ','',regex=False)\n",
    "single_dicts_copy.iloc[:,0] = single_dicts_copy.iloc[:,0].str.replace(\"\\\"\",'',regex=False)\n",
    "single_dicts_copy.columns = ['dictionary']\n",
    "print(single_dicts_copy.iloc[0,0],'\\n')\n",
    "print(single_dicts_copy.iloc[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_lst = list(single_dicts_copy.iloc[:,0].values)\n",
    "info = []\n",
    "i=0\n",
    "\n",
    "for i in range(len(single_dicts_copy.iloc[:,0])):\n",
    "    item = long_lst[i].split(\",\")\n",
    "    info.append(item)\n",
    "\n",
    "print(len(info),'\\n')\n",
    "info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_names = []\n",
    "i=0\n",
    "# initializing substring\n",
    "subs = 'screen_name'\n",
    "\n",
    "for i in range(len(info)):\n",
    "    item = list(filter(lambda x: subs in x, info[i]))\n",
    "    screen_names.append(item)\n",
    "\n",
    "print(len(screen_names),'\\n')\n",
    "screen_names[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "i=0\n",
    "# initializing substring\n",
    "subs = 'id:'\n",
    "\n",
    "for i in range(len(info)):\n",
    "    item = list(filter(lambda x: subs in x, info[i]))\n",
    "    ids.append(item)\n",
    "\n",
    "print(len(ids),'\\n')\n",
    "ids[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in screen_names for item in sublist]\n",
    "screen_names_s = pd.Series(flat_list)\n",
    "screen_names_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in ids for item in sublist]\n",
    "ids_s = pd.Series(flat_list)\n",
    "ids_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"id\": ids_s,\n",
    "        \"screen_name\": screen_names_s}\n",
    "\n",
    "persons_df = pd.concat(data,axis=1)\n",
    "persons_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove identifiers\n",
    "persons_df['id'] = persons_df['id'].str.replace('id:','',regex=False)\n",
    "persons_df['screen_name'] = persons_df['screen_name'].str.replace('screen_name:','',regex=False)\n",
    "persons_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to R for wordcloud\n",
    "persons_df.to_csv('C:/Users/CT/Documents/GitHub/Delta-Analytics-2021-CT-Project/Tweeps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "persons <- read.csv('C:/Users/CT/Documents/GitHub/Delta-Analytics-2021-CT-Project/Tweeps.csv')\n",
    "\n",
    "Tweep_count <- as.data.frame(table(persons$screen_name))\n",
    "\n",
    "viz2 <- wordcloud2(Tweep_count, size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-dylan",
   "metadata": {},
   "source": [
    "![](viz2.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "head(Tweep_count[order(Tweep_count$Freq, decreasing = TRUE),],n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-agent",
   "metadata": {},
   "source": [
    "Based on the result above, the most mentioned 'persons' over the past year on Twitter were \n",
    "\n",
    "* political institutions such as `@statehousekenya`, `@dcikenya` and `@nassemblyke`,\n",
    "* a betting firm, `@safebetske`,\n",
    "* politicians such as `@railaodinga` (the leader of Kenyan opposition parties) and `@williamsruto` (the current deputy vice president) as well as\n",
    "* media houses such as `@citizentvkenya`, `@ntvkenya` and `@classic105kenya` and\n",
    "* an inspiration account, `@dodzweit` (A reverend who posts inspirational and at times Christian content)\n",
    "\n",
    "A possible explanation for this phenomenon is that 2022 will be an election year, and the two political leaders are running against each other and campaigning online.\n",
    "\n",
    "However,\n",
    "\n",
    "* `@dodzweit` was mentioned only by the church s/he pastors, `@cotchurchhq`, primarily in short summaries of his/her talks.\n",
    "* `@safebetske` was mentioned only by a forex trader, `@theforexguyke`, primarily in retweets.\n",
    "\n",
    "## MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "model_data <- cbind.data.frame(eng_tweets$id, eng_tweets$conversation_id,\n",
    "                               eng_tweets$date, eng_tweets$time,\n",
    "                               eng_tweets$user_id, eng_tweets$tweet,\n",
    "                               eng_tweets$mentions, eng_tweets$hashtags)\n",
    "write.csv(model_data, 'eng_model_data.csv')\n",
    "model_data[1:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('eng_model_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = data.copy()\n",
    "\n",
    "print(model_data.dtypes)\n",
    "\n",
    "model_data['eng_tweets$id'] = model_data['eng_tweets$id'].astype('category')\n",
    "model_data['eng_tweets$conversation_id'] = model_data['eng_tweets$conversation_id'].astype('category')\n",
    "model_data['eng_tweets$user_id'] = model_data['eng_tweets$user_id'].astype('category')\n",
    "model_data['timestamp'] = model_data['eng_tweets$date'] + ' ' + model_data['eng_tweets$time']\n",
    "model_data['timestamp'] = pd.to_datetime(model_data['timestamp'])\n",
    "model_data = model_data.drop(['Unnamed: 0', 'eng_tweets$date', 'eng_tweets$time',\n",
    "                              'eng_tweets$mentions', 'eng_tweets$hashtags'],1)\n",
    "\n",
    "print('\\n', model_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.columns = ['tweet_id', 'conversation_id', 'user_id', 'tweet', 'timestamp']\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!pip install python-Levenshtein\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument #tokenize and tag each tweet\n",
    "model_data['tagged_tweets'] = [TaggedDocument(doc.split(' '), [i]) \n",
    "             for i, doc in enumerate(model_data.tweet)]#display the tagged docs\n",
    "\n",
    "#model_data = model_data.drop(['tweet'],1) #drop the raw tweets\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-sharp",
   "metadata": {},
   "source": [
    "Based on the data above, unsupervised machine learning will be applied to:\n",
    "* tranform tweets into numerical vectors (learn more [here](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)) using [Doc2Vec](https://towardsdatascience.com/how-to-vectorize-text-in-dataframes-for-nlp-tasks-3-simple-techniques-82925a5600db)\n",
    "* use k-means to [classify the tweets](https://towardsdatascience.com/unsupervised-sentiment-analysis-a38bf1906483), constructing a label in the process.\n",
    "* use Catboost to [predict the constructed label](https://towardsdatascience.com/unconventional-sentiment-analysis-bert-vs-catboost-90645f2437a9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the vectors into numerical vectors\n",
    "model = Doc2Vec(dm=1, vector_size=32, min_count=1, workers=8, epochs = 20) #instantiate model\n",
    "model.build_vocab(model_data['tagged_tweets']) #build vocab\n",
    "model.train(model_data['tagged_tweets'], total_examples=model.corpus_count,\n",
    "            epochs=model.epochs) # train the vectorization model\n",
    "\n",
    "#generate vectors\n",
    "tweet_vec = [model.infer_vector((model_data['tweet'][i].split(' '))) \n",
    "            for i in range(0,len(model_data['tweet']))]\n",
    "tweet_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_vector = np.array(tweet_vec).tolist() #Create a list of lists\n",
    "model_data['tweet_vector'] = tweet_vector #set list to dataframe column\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(tweet_vec) # classify data into 2 pools based on document vectors\n",
    "model_data['label'] = pd.Series(kmeans.labels_)\n",
    "\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets labeled 0:\n",
    "i=0\n",
    "for i in range(26):\n",
    "    if model_data['label'][i] == 0:\n",
    "        print(model_data.tweet[i], '\\n', '\\n')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets labeled 1:\n",
    "i=0\n",
    "for i in range(26):\n",
    "    if model_data['label'][i] == 1:\n",
    "        print(model_data.tweet[i], '\\n', '\\n')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-microphone",
   "metadata": {},
   "source": [
    "The results above indicate that there is misuse of hashtags to push products and there is no clear positive sentiment or negative sentiment in any of the classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

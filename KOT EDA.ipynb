{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "moderate-episode",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis for the Delta Analytics Teaching Fellowship\n",
    "\n",
    "**Author:** *Cynthia Thinwa*\n",
    "\n",
    "## INTRODUCTION\n",
    "\n",
    "### DATA HANDLING PRACTICES:\n",
    "\n",
    "* Based on Twitter API best practice, the actual data will not be shared, only Twitter's tweet IDs for future reference\n",
    "* The data will be cleaned to remove personally identifiable information like emails and phone numbers\n",
    "* Content published on social media platforms belongs to the public domain with the various authors having given consent\n",
    "\n",
    "Data collection was done from a Microsoft command line running\n",
    "\n",
    "`twint -s %23kot --since \"2020-06-01 03:00:00\" --until \"2021-06-01 03:00:00\" -o kotdata.csv --csv`\n",
    "\n",
    "## EXPLORATORY DATA ANALYSIS\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The raw data was loaded as follows, with the following characteristics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import plotly.express as px # for data visualization\n",
    "#!pip install gensim\n",
    "#!pip install python-Levenshtein\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument #tokenize and tag each tweet\n",
    "#!pip install sklearn\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(dplyr)\n",
    "library(wordcloud)\n",
    "library(RColorBrewer)\n",
    "library(rtweet)\n",
    "library(tidytext)\n",
    "library(ggplot2)\n",
    "library(wordcloud2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-thirty",
   "metadata": {},
   "source": [
    "1. The number of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "url <- \"C:/storage/Personal drive backup/Career/Post-Masters/Delta Analytics Teaching Fellowship/EDA/2.kotdata.csv\"\n",
    "DATFdata <- read.delim(url)\n",
    "dim(DATFdata)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-characteristic",
   "metadata": {},
   "source": [
    "\n",
    "2. The number of unique conversations had:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "DATFdata$conversation_id <- factor(DATFdata$conversation_id)\n",
    "DATFdata$id <- factor(DATFdata$id)\n",
    "\n",
    "dim(as.data.frame(table(DATFdata$conversation_id)))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-system",
   "metadata": {},
   "source": [
    "\n",
    "3. The number of unique users speaking:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "DATFdata$user_id <- factor(DATFdata$user_id)\n",
    "\n",
    "dim(as.data.frame(table(DATFdata$user_id)))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-highlight",
   "metadata": {},
   "source": [
    "\n",
    "4. The most frequent language of posting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "lang <- as.data.frame(table(DATFdata$language))\n",
    "colnames(lang) <- c('Language','Frequency')\n",
    "head(lang[order(lang$Freq, decreasing = TRUE),],n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-luxembourg",
   "metadata": {},
   "source": [
    "\n",
    "5. The date on which most tweets were posted (tweets were from 1st June 2020 UTC+3 upto 1st June 2021 UTC+3): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "dates <- as.data.frame(table(DATFdata$date))\n",
    "colnames(dates) <- c('Date','Frequency')\n",
    "head(dates[order(dates$Freq, decreasing = TRUE),],n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-arlington",
   "metadata": {},
   "source": [
    "\n",
    "### Text transformation\n",
    "\n",
    "Text cleaning was as follows, using `eng_tweets$tweet[4]` as an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Get organic tweets first; found that all tweets were organic!\n",
    "\n",
    "# get only English ones:\n",
    "eng_tweets <- DATFdata[DATFdata$language=='en',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Remove funny symbols\n",
    "eng_tweets$tweet <- iconv(eng_tweets$tweet, from = 'UTF-8', to = 'ISO-8859-1', sub = '')\n",
    "eng_tweets$tweet <- iconv(eng_tweets$tweet, from = 'ISO-8859-1', to = 'UTF-8', sub = '')\n",
    "\n",
    "eng_tweets$tweet[1:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Get organic tweets first; found that all tweets were organic!\n",
    "\n",
    "# get only English ones:\n",
    "eng_tweets <- DATFdata[DATFdata$language=='en',]; print(eng_tweets$tweet[4])\n",
    "\n",
    "# Remove funny symbols\n",
    "eng_tweets$tweet <- iconv(eng_tweets$tweet, from = 'UTF-8', to = 'ISO-8859-1', sub = ''); print(eng_tweets$tweet[4]); cat('\\n')\n",
    "\n",
    "eng_tweets$tweet <- iconv(eng_tweets$tweet, from = 'ISO-8859-1', to = 'UTF-8', sub = ''); print(eng_tweets$tweet[4]); cat('\\n')\n",
    "\n",
    "eng_tweets$tweet <- gsub(\"https\\\\S*\", \"\", eng_tweets$tweet); print(eng_tweets$tweet[4]); cat('\\n') #remove urls\n",
    "\n",
    "eng_tweets$tweet <- gsub(\"@\", \"\", eng_tweets$tweet); print(eng_tweets$tweet[4]); cat('\\n') #remove mentions symbol\n",
    "\n",
    "eng_tweets$tweet <- gsub(\"#*\", \"\", eng_tweets$tweet); print(eng_tweets$tweet[4]); cat('\\n') #remove hashtags symbol\n",
    "\n",
    "eng_tweets$tweet <- gsub(\"[\\r\\n]\", \" \", eng_tweets$tweet); print(eng_tweets$tweet[4]); cat('\\n') #remove newline characters\n",
    "\n",
    "#(we have separate columns with the details)\n",
    "# Punctuation was managed as follows:\n",
    "eng_tweets$tweet <- gsub(\"'\", \"\", eng_tweets$tweet); print(eng_tweets$tweet[4]); cat('\\n')\n",
    "\n",
    "eng_tweets$tweet <- gsub(\"[[:punct:]]\", \" \", eng_tweets$tweet); print(eng_tweets$tweet[4]); cat('\\n')\n",
    "\n",
    "eng_tweets$tweet <- gsub(\"amp\", \"\", eng_tweets$tweet); print(eng_tweets$tweet[4]); cat('\\n') # remove ampersands\n",
    "\n",
    "# Finally, everything was made lowercase\n",
    "eng_tweets$tweet <- tolower(eng_tweets$tweet); print(eng_tweets$tweet[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Tokenize words\n",
    "Words <- eng_tweets %>%\n",
    "  select(tweet) %>%\n",
    "  unnest_tokens(word, tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-generator",
   "metadata": {},
   "source": [
    "\n",
    "### Word Frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "Words %>% # gives you a bar chart of the most frequent words found in the tweets\n",
    "  count(word, sort = TRUE) %>%\n",
    "  top_n(15) %>%\n",
    "  mutate(word = reorder(word, n)) %>%\n",
    "  ggplot(aes(x = word, y = n)) +\n",
    "  geom_col() +\n",
    "  xlab(NULL) +\n",
    "  coord_flip() +\n",
    "  labs(y = \"Count\",\n",
    "       x = \"Unique words\",\n",
    "       title = \"Most frequent words found in the #KOT tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# remove stop words\n",
    "Words <- Words %>%\n",
    "  anti_join(stop_words)\n",
    "\n",
    "Words %>% # gives you a bar chart of the most frequent words found in the tweets\n",
    "  count(word, sort = TRUE) %>%\n",
    "  top_n(15) %>%\n",
    "  mutate(word = reorder(word, n)) %>%\n",
    "  ggplot(aes(x = word, y = n)) +\n",
    "  geom_col() +\n",
    "  xlab(NULL) +\n",
    "  coord_flip() +\n",
    "  labs(y = \"Count\",\n",
    "       x = \"Unique words\",\n",
    "       title = \"Most frequent words found in the #KOT tweets\",\n",
    "       subtitle = \"Stop words removed from the list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-alexander",
   "metadata": {},
   "source": [
    "\n",
    "### Hashtag Frequency\n",
    "\n",
    "All tweets have the hashtag `#KOT`, but we are interested in what else this community talks about, so this hashtag will have to be removed from our visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "eng_tweets$hashtags <- as.character(eng_tweets$hashtags)\n",
    "\n",
    "eng_tweets$hashtags <- gsub(\"[[:punct:]]\", \"\", eng_tweets$hashtags)\n",
    "\n",
    "# every hashtag was made lowercase\n",
    "eng_tweets$hashtags <- tolower(eng_tweets$hashtags)\n",
    "\n",
    "# and #KOT was removed before tokenization\n",
    "eng_tweets$hashtags <- gsub(\"kot\", \"\", eng_tweets$hashtags)\n",
    "\n",
    "Hashtags <- eng_tweets %>%\n",
    "  select(hashtags) %>%\n",
    "  unnest_tokens(word, hashtags)\n",
    "\n",
    "Hashtags_count <- as.data.frame(table(Hashtags$word))\n",
    "\n",
    "viz1 <- wordcloud2(Hashtags_count, size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-winter",
   "metadata": {},
   "source": [
    "![](viz1.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "df <- head(Hashtags_count[order(Hashtags_count$Freq, decreasing = TRUE),], n=15)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-authority",
   "metadata": {},
   "source": [
    "Based on the results above\n",
    "\n",
    "* platform (`#loyals`),\n",
    "* patriotism (`#kenya` and `#nairobi`),\n",
    "* radio (`#mainaandkingangi` and `#teamclassic`),\n",
    "* political trends (`#bbinonsense` and `#punguzamizigo`)\n",
    "* job ads(`#ikokazike` and `#ikokazi`) and\n",
    "* cats (`#cat` and `#gato`)\n",
    "\n",
    "took centre stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-indonesia",
   "metadata": {},
   "source": [
    "\n",
    "### People of Influence\n",
    "\n",
    "In this area, we are interested in identifying people mentioned most frequently.\n",
    "\n",
    "The text, using `eng_tweets$mentions[4]` as an example was first converted as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "eng_tweets$mentions[4]\n",
    "# Remove funny symbols\n",
    "eng_tweets$mentions <- iconv(eng_tweets$mentions, from = 'UTF-8', to = 'UTF-8');eng_tweets$mentions[4]\n",
    "\n",
    "eng_tweets$mentions <- stringr::str_replace_all(\n",
    "  eng_tweets$mentions, \"\\'\", \"\\\"\")\n",
    "eng_tweets$mentions <- gsub(\"[\", \"\", eng_tweets$mentions, fixed = TRUE)\n",
    "eng_tweets$mentions <- gsub(\"]\", \"\", eng_tweets$mentions, fixed = TRUE)\n",
    "\n",
    "df <- as.data.frame(eng_tweets$mentions); dim(df)\n",
    "df[df == \"\"] <- NA  \n",
    "df <- na.omit(df); dim(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get file location\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "write.csv(df$`eng_tweets$mentions`,'C:/Users/CT/Documents/GitHub/Delta-Analytics-2021-CT-Project/dicts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = pd.read_csv('dicts.csv')\n",
    "dicts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the column\n",
    "dicts = dicts.drop(labels='Unnamed: 0', axis=1)\n",
    "dicts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dicts_copy = dicts.copy()\n",
    "single_dicts_copy.iloc[:,0] = single_dicts_copy.iloc[:,0].str.replace('{','',regex=False)\n",
    "single_dicts_copy.iloc[:,0] = single_dicts_copy.iloc[:,0].str.replace('}','',regex=False)\n",
    "single_dicts_copy.iloc[:,0] = single_dicts_copy.iloc[:,0].str.replace(' ','',regex=False)\n",
    "single_dicts_copy.iloc[:,0] = single_dicts_copy.iloc[:,0].str.replace(\"\\\"\",'',regex=False)\n",
    "single_dicts_copy.columns = ['dictionary']\n",
    "print(single_dicts_copy.iloc[0,0],'\\n')\n",
    "print(single_dicts_copy.iloc[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_lst = list(single_dicts_copy.iloc[:,0].values)\n",
    "info = []\n",
    "i=0\n",
    "\n",
    "for i in range(len(single_dicts_copy.iloc[:,0])):\n",
    "    item = long_lst[i].split(\",\")\n",
    "    info.append(item)\n",
    "\n",
    "print(len(info),'\\n')\n",
    "info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_names = []\n",
    "i=0\n",
    "# initializing substring\n",
    "subs = 'screen_name'\n",
    "\n",
    "for i in range(len(info)):\n",
    "    item = list(filter(lambda x: subs in x, info[i]))\n",
    "    screen_names.append(item)\n",
    "\n",
    "print(len(screen_names),'\\n')\n",
    "screen_names[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "i=0\n",
    "# initializing substring\n",
    "subs = 'id:'\n",
    "\n",
    "for i in range(len(info)):\n",
    "    item = list(filter(lambda x: subs in x, info[i]))\n",
    "    ids.append(item)\n",
    "\n",
    "print(len(ids),'\\n')\n",
    "ids[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in screen_names for item in sublist]\n",
    "screen_names_s = pd.Series(flat_list)\n",
    "screen_names_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in ids for item in sublist]\n",
    "ids_s = pd.Series(flat_list)\n",
    "ids_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"id\": ids_s,\n",
    "        \"screen_name\": screen_names_s}\n",
    "\n",
    "persons_df = pd.concat(data,axis=1)\n",
    "persons_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove identifiers\n",
    "persons_df['id'] = persons_df['id'].str.replace('id:','',regex=False)\n",
    "persons_df['screen_name'] = persons_df['screen_name'].str.replace('screen_name:','',regex=False)\n",
    "persons_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to R for wordcloud\n",
    "persons_df.to_csv('C:/Users/CT/Documents/GitHub/Delta-Analytics-2021-CT-Project/Tweeps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "persons <- read.csv('C:/Users/CT/Documents/GitHub/Delta-Analytics-2021-CT-Project/Tweeps.csv')\n",
    "\n",
    "Tweep_count <- as.data.frame(table(persons$screen_name))\n",
    "\n",
    "viz2 <- wordcloud2(Tweep_count, size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-lexington",
   "metadata": {},
   "source": [
    "![](viz2.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "df <- head(Tweep_count[order(Tweep_count$Freq, decreasing = TRUE),],n=10); print(df)\n",
    "\n",
    "ggplot(data=df, aes(x=\"\", y=Freq, fill=Var1)) +\n",
    "  geom_bar(stat=\"identity\", width=1) +\n",
    "  coord_polar(\"y\", start=0) +\n",
    "  labs(x = NULL, y = NULL, fill = NULL) +\n",
    "  theme_classic() +\n",
    "  theme(axis.line = element_blank(),\n",
    "        axis.text = element_blank(),\n",
    "        axis.ticks = element_blank()) +\n",
    "  scale_fill_brewer(palette=\"Paired\") +\n",
    "  geom_text(aes(label = paste0(round(Freq/sum(Freq)*100, 1), \"%\")), position = position_stack(vjust=0.5)) +\n",
    "  labs(title = \"Top 10 mentioned accounts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-waste",
   "metadata": {},
   "source": [
    "Based on the result above, the most mentioned 'persons' over the past year on Twitter were \n",
    "\n",
    "* political institutions such as `@statehousekenya`, `@dcikenya` and `@nassemblyke`,\n",
    "* a betting firm, `@safebetske`,\n",
    "* politicians such as `@railaodinga` (the leader of Kenyan opposition parties) and `@williamsruto` (the current deputy vice president) as well as\n",
    "* media houses such as `@citizentvkenya`, `@ntvkenya` and `@classic105kenya` and\n",
    "* an inspiration account, `@dodzweit` (A reverend who posts inspirational and at times Christian content)\n",
    "\n",
    "A possible explanation for this phenomenon is that 2022 will be an election year, and the two political leaders are running against each other and campaigning online.\n",
    "\n",
    "However,\n",
    "\n",
    "* `@dodzweit` was mentioned only by the church s/he pastors, `@cotchurchhq`, primarily in short summaries of his/her talks.\n",
    "* `@safebetske` was mentioned only by a forex trader, `@theforexguyke`, primarily in retweets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-regression",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING \\& MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "model_data <- cbind.data.frame(eng_tweets$id, eng_tweets$conversation_id,\n",
    "                               eng_tweets$date, eng_tweets$time,\n",
    "                               eng_tweets$user_id, eng_tweets$tweet,\n",
    "                               eng_tweets$mentions, eng_tweets$hashtags)\n",
    "write.csv(model_data, 'eng_model_data.csv')\n",
    "model_data[1:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('eng_model_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = data.copy()\n",
    "\n",
    "print(model_data.dtypes)\n",
    "\n",
    "model_data['eng_tweets$id'] = model_data['eng_tweets$id'].astype('category')\n",
    "model_data['eng_tweets$conversation_id'] = model_data['eng_tweets$conversation_id'].astype('category')\n",
    "model_data['eng_tweets$user_id'] = model_data['eng_tweets$user_id'].astype('category')\n",
    "model_data['timestamp'] = model_data['eng_tweets$date'] + ' ' + model_data['eng_tweets$time']\n",
    "model_data['timestamp'] = pd.to_datetime(model_data['timestamp'])\n",
    "model_data = model_data.drop(['Unnamed: 0', 'eng_tweets$date', 'eng_tweets$time',\n",
    "                              'eng_tweets$mentions', 'eng_tweets$hashtags'],1)\n",
    "\n",
    "print('\\n', model_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.columns = ['tweet_id', 'conversation_id', 'user_id', 'tweet', 'timestamp']\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data['tagged_tweets'] = [TaggedDocument(doc.split(' '), [i]) \n",
    "             for i, doc in enumerate(model_data.tweet)]#display the tagged docs\n",
    "\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-acrobat",
   "metadata": {},
   "source": [
    "Based on the data above, unsupervised machine learning will be applied to:\n",
    "* tranform tweets into numerical vectors (learn more [here](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)) using [Doc2Vec](https://towardsdatascience.com/how-to-vectorize-text-in-dataframes-for-nlp-tasks-3-simple-techniques-82925a5600db)\n",
    "* use k-means to [classify the tweets](https://towardsdatascience.com/unsupervised-sentiment-analysis-a38bf1906483), constructing a label in the process.\n",
    "* use Catboost to [predict the constructed label](https://towardsdatascience.com/unconventional-sentiment-analysis-bert-vs-catboost-90645f2437a9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the vectors into numerical vectors\n",
    "model = Doc2Vec(dm=1, vector_size=32, min_count=1, workers=8, epochs = 20) #instantiate model\n",
    "model.build_vocab(model_data['tagged_tweets']) #build vocab\n",
    "model.train(model_data['tagged_tweets'], total_examples=model.corpus_count,\n",
    "            epochs=model.epochs) # train the vectorization model\n",
    "\n",
    "#generate vectors\n",
    "tweet_vec = [model.infer_vector((model_data['tweet'][i].split(' '))) \n",
    "            for i in range(0,len(model_data['tweet']))]\n",
    "tweet_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_vec[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-hunter",
   "metadata": {},
   "source": [
    "### Using `tweet_vector` as the only feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_data0 = pd.DataFrame()\n",
    "kmeans1 = KMeans(n_clusters=2, random_state=0).fit(tweet_vec) # classify data into 2 pools based on document vectors\n",
    "kmeans_data0['label'] = pd.Series(kmeans1.labels_)\n",
    "kmeans_data0['tweet_vector'] = tweet_vec #set list to dataframe column\n",
    "kmeans_data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets labeled 0:\n",
    "i=0\n",
    "for i in range(10):\n",
    "    if kmeans_data0['label'][i] == 0:\n",
    "        print(model_data.tweet[i], '\\n', '\\n')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets labeled 1:\n",
    "i=0\n",
    "for i in range(10):\n",
    "    if kmeans_data0['label'][i] == 1:\n",
    "        print(model_data.tweet[i], '\\n', '\\n')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-frank",
   "metadata": {},
   "source": [
    "### Using vectorized tweet elements, `user_id` and `conversation_id` as the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_vector = np.array(tweet_vec).tolist() #Create a list of lists\n",
    "\n",
    "user_idLIST = []\n",
    "for id in model_data.user_id:\n",
    "    i=0\n",
    "    for i in range(32):\n",
    "        user_idLIST.append(id)\n",
    "        i+=1\n",
    "print('\\n',len(user_idLIST),'\\n')\n",
    "print(user_idLIST[0:32],'\\n','\\n')\n",
    "\n",
    "conversation_idLIST = []\n",
    "for id in model_data.conversation_id:\n",
    "    i=0\n",
    "    for i in range(32):\n",
    "        conversation_idLIST.append(id)\n",
    "        i+=1\n",
    "print(len(conversation_idLIST),'\\n')\n",
    "print(conversation_idLIST[0:32],'\\n','\\n')\n",
    "\n",
    "#tweet_vectorLIST = kmeans_data.tweet_vector.flatten()\n",
    "#print(kmeans_data.tweet_vector[0])\n",
    "#tweet_vectorLIST[1:32]\n",
    "print(len(np.array(tweet_vec).flatten()),'\\n')\n",
    "print(list(np.array(tweet_vec).flatten())[0:32],'\\n')\n",
    "print(list(np.array(tweet_vec).flatten())[32:64],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_data1 = pd.DataFrame()\n",
    "kmeans_data1['user_idLIST'] = pd.Series(user_idLIST)\n",
    "kmeans_data1['conversation_idLIST'] = pd.Series(conversation_idLIST)\n",
    "kmeans_data1['tweet_vectorLIST'] = pd.Series(np.array(tweet_vec).flatten())\n",
    "\n",
    "kmeans2 = KMeans(n_clusters=2, random_state=1).fit(kmeans_data1) # classify data into 2 pools based on document vectors\n",
    "kmeans_data1['label'] = pd.Series(kmeans2.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_data1.head(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-reverse",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_data1.iloc[32:64,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "labels=[]\n",
    "for i in range(27504):\n",
    "    j= (i*32)\n",
    "    labels.append(kmeans_data1.label[j])\n",
    "    i+=1\n",
    "\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_data1_results = pd.DataFrame()\n",
    "kmeans_data1_results['label'] = labels\n",
    "# Tweets labeled 0:\n",
    "i=0\n",
    "for i in range(10):\n",
    "    if kmeans_data1_results['label'][i] == 0:\n",
    "        print(model_data.tweet[i], '\\n')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets labeled 1:\n",
    "i=0\n",
    "for i in range(10):\n",
    "    if kmeans_data1_results['label'][i] == 1:\n",
    "        print(model_data.tweet[i], '\\n')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-terminology",
   "metadata": {},
   "source": [
    "### Using alternative features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-sample",
   "metadata": {},
   "source": [
    "The results above indicate that there is misuse of hashtags to push products and there is no clear positive sentiment or negative sentiment in any of the classes.\n",
    "\n",
    "What if we derived more features from the data to include:\n",
    "* presence of advertising (inclusion of \" com\" - previously .com and phone numbers - 10-digit numbers)\n",
    "* presence of political institutions or persons\n",
    "* presence of words in sheng and english that indicate like and dislike"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-coast",
   "metadata": {},
   "source": [
    "### Using positive and negative dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_data2 = pd.DataFrame()\n",
    "\n",
    "# Positive Sentiment\n",
    "kmeans_data2['i_like'] = model_data.tweet.str.contains(' i like ')\n",
    "kmeans_data2['love'] = model_data.tweet.str.contains(' love ')\n",
    "kmeans_data2['penda'] = model_data.tweet.str.contains('penda ') # includes napenda, tunapenda etc.\n",
    "kmeans_data2['agree'] = model_data.tweet.str.contains(' agree ')\n",
    "kmeans_data2['even_me'] = model_data.tweet.str.contains(' even me ')\n",
    "kmeans_data2['hata_mimi'] = model_data.tweet.str.contains(' hata mimi ')\n",
    "kmeans_data2['pia_mimi'] = model_data.tweet.str.contains(' pia mimi ')\n",
    "kmeans_data2['i_can'] = model_data.tweet.str.contains(' i can ')\n",
    "kmeans_data2['weza'] = model_data.tweet.str.contains('weza ')\n",
    "kmeans_data2['niko_easy'] = model_data.tweet.str.contains(' niko easy ')\n",
    "kmeans_data2['sawasawa'] = model_data.tweet.str.contains(' sawasawa ')\n",
    "kmeans_data2['ni_poa'] = model_data.tweet.str.contains(' ni poa ')\n",
    "kmeans_data2['want'] = model_data.tweet.str.contains(' want ')\n",
    "kmeans_data2['am_happy'] = model_data.tweet.str.contains(' am happy ')\n",
    "kmeans_data2['im_happy'] = model_data.tweet.str.contains(' im happy ')\n",
    "kmeans_data2['nafurahi'] = model_data.tweet.str.contains('nafurahi ')\n",
    "kmeans_data2['mefurahi'] = model_data.tweet.str.contains('mefurahi ') # nimetulia, tumetulia etc.\n",
    "kmeans_data2['peace'] = model_data.tweet.str.contains(' peace')\n",
    "kmeans_data2['natulia'] = model_data.tweet.str.contains('natulia ')\n",
    "kmeans_data2['metulia'] = model_data.tweet.str.contains('metulia ') # nimetulia, tumetulia etc.\n",
    "kmeans_data2['grateful'] = model_data.tweet.str.contains(' grateful ')\n",
    "kmeans_data2['okay'] = model_data.tweet.str.contains(' okay ')\n",
    "kmeans_data2['thankful'] = model_data.tweet.str.contains(' thankful ')\n",
    "kmeans_data2['shukuru'] = model_data.tweet.str.contains('shukuru ') # nashukuru, twashukuru etc.\n",
    "kmeans_data2['shukran'] = model_data.tweet.str.contains('shukran ')\n",
    "kmeans_data2['asante'] = model_data.tweet.str.contains(' asante')\n",
    "\n",
    "# Negative Sentiment\n",
    "kmeans_data2['dislike'] = model_data.tweet.str.contains(' dislike ')\n",
    "kmeans_data2['dont_like'] = model_data.tweet.str.contains(' dont like ')\n",
    "kmeans_data2['pendi'] = model_data.tweet.str.contains('pendi ') # includes sipendi, hatupendi etc.\n",
    "kmeans_data2['disagree'] = model_data.tweet.str.contains(' disagree ')\n",
    "kmeans_data2['dont_agree'] = model_data.tweet.str.contains(' dont agree ')\n",
    "kmeans_data2['i_kent'] = model_data.tweet.str.contains(' i kent ')\n",
    "kmeans_data2['i_cant'] = model_data.tweet.str.contains(' i cant ')\n",
    "kmeans_data2['siwezi'] = model_data.tweet.str.contains(' siwezi ')\n",
    "kmeans_data2['ngori'] = model_data.tweet.str.contains(' ngori ') # ni ngori i.e. it's hard\n",
    "kmeans_data2['siko_sawa'] = model_data.tweet.str.contains(' siko sawa ')\n",
    "kmeans_data2['si_poa'] = model_data.tweet.str.contains(' si poa ')\n",
    "kmeans_data2['dont_want'] = model_data.tweet.str.contains(' dont want ')\n",
    "kmeans_data2['staki'] = model_data.tweet.str.contains(' staki ')\n",
    "kmeans_data2['sitaki'] = model_data.tweet.str.contains(' sitaki ')\n",
    "kmeans_data2['hapana'] = model_data.tweet.str.contains(' hapana ')\n",
    "kmeans_data2['kataa'] = model_data.tweet.str.contains('kataa ') # ninakataa, tumekataa etc.\n",
    "kmeans_data2['jafurahi'] = model_data.tweet.str.contains('jafurahi ') # sijafurahi, hatujafurahi etc.\n",
    "kmeans_data2['sifurahii'] = model_data.tweet.str.contains(' sifurahii ')\n",
    "kmeans_data2['angry'] = model_data.tweet.str.contains(' angry ')\n",
    "kmeans_data2['frustrated'] = model_data.tweet.str.contains(' frustrated ')\n",
    "kmeans_data2['annoyed'] = model_data.tweet.str.contains(' annoyed ')\n",
    "kmeans_data2['nimejam'] = model_data.tweet.str.contains(' nimejam ')\n",
    "kmeans_data2['kasirika'] = model_data.tweet.str.contains('kasirika ') # wamekasirika, ninakasirika etc.\n",
    "kmeans_data2['kasirishwa'] = model_data.tweet.str.contains('kasirishwa ') # wamekasirishwa, ninakasirishwa etc.\n",
    "kmeans_data2['jatulia'] = model_data.tweet.str.contains('jatulia ') # sijatulia, hatujatulia etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_data2 = kmeans_data2.astype('int16')\n",
    "kmeans_data2['positive'] = sum([kmeans_data2['i_like'], kmeans_data2['love'], kmeans_data2['penda'], kmeans_data2['agree'],\n",
    "                                kmeans_data2['even_me'], kmeans_data2['hata_mimi'], kmeans_data2['pia_mimi'],\n",
    "                                kmeans_data2['i_can'], kmeans_data2['weza'], kmeans_data2['niko_easy'],\n",
    "                                kmeans_data2['sawasawa'], kmeans_data2['ni_poa'], kmeans_data2['want'],\n",
    "                                kmeans_data2['am_happy'], kmeans_data2['im_happy'], kmeans_data2['nafurahi'],\n",
    "                                kmeans_data2['mefurahi'], kmeans_data2['peace'], kmeans_data2['natulia'],\n",
    "                                kmeans_data2['metulia'], kmeans_data2['grateful'], kmeans_data2['okay'],\n",
    "                                kmeans_data2['thankful'], kmeans_data2['shukuru'], kmeans_data2['shukran'],\n",
    "                                kmeans_data2['asante'] ])\n",
    "kmeans_data2['is_positive'] = kmeans_data2['positive'] > 0\n",
    "\n",
    "kmeans_data2['negative'] = -sum([kmeans_data2['dislike'], kmeans_data2['dont_like'], kmeans_data2['pendi'],\n",
    "                                 kmeans_data2['disagree'], kmeans_data2['dont_agree'], kmeans_data2['i_kent'],\n",
    "                                 kmeans_data2['i_cant'], kmeans_data2['siwezi'], kmeans_data2['ngori'],\n",
    "                                 kmeans_data2['siko_sawa'], kmeans_data2['si_poa'], kmeans_data2['dont_want'],\n",
    "                                 kmeans_data2['staki'], kmeans_data2['sitaki'], kmeans_data2['hapana'],\n",
    "                                 kmeans_data2['kataa'], kmeans_data2['jafurahi'], kmeans_data2['sifurahii'],\n",
    "                                 kmeans_data2['angry'], kmeans_data2['frustrated'], kmeans_data2['annoyed'],\n",
    "                                 kmeans_data2['nimejam'], kmeans_data2['kasirika'], kmeans_data2['kasirishwa'],\n",
    "                                 kmeans_data2['jatulia'] ])\n",
    "kmeans_data2['is_negative'] = kmeans_data2['negative'] < 0\n",
    "\n",
    "print(kmeans_data2.shape)\n",
    "kmeans_data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_data2 = kmeans_data2.drop(['i_like', 'love', 'penda', 'agree', 'even_me', 'hata_mimi',\n",
    "                                  'pia_mimi', 'i_can', 'weza', 'niko_easy', 'sawasawa',\n",
    "                                  'ni_poa', 'want', 'am_happy', 'im_happy', 'nafurahi',\n",
    "                                  'mefurahi', 'peace', 'natulia', 'metulia', 'grateful',\n",
    "                                  'okay', 'thankful', 'shukuru', 'shukran', 'asante'],axis=1)\n",
    "print(kmeans_data2.shape)\n",
    "kmeans_data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_data2 = kmeans_data2.drop(['dislike', 'dont_like', 'pendi', 'disagree', 'dont_agree',\n",
    "                                  'i_kent', 'i_cant', 'siwezi', 'ngori', 'siko_sawa',\n",
    "                                  'si_poa', 'dont_want', 'staki', 'sitaki', 'hapana',\n",
    "                                 'kataa', 'jafurahi', 'sifurahii', 'angry', 'frustrated',\n",
    "                                  'annoyed', 'nimejam', 'kasirika', 'kasirishwa', 'jatulia'],axis=1)\n",
    "print(kmeans_data2.shape)\n",
    "kmeans_data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(kmeans_data2['is_positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(kmeans_data2['is_negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_data2['user_id'] = model_data.user_id\n",
    "kmeans_data2['conversation_id'] = model_data.conversation_id\n",
    "kmeans_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_data2 = kmeans_data2[np.logical_or(kmeans_data2['is_positive'] == True, kmeans_data2['is_negative'] == True)]\n",
    "\n",
    "for column in kmeans_data2.columns:\n",
    "    print(column,': ',np.sum(kmeans_data2[column].isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans3 = KMeans(n_clusters=2, random_state=3).fit(kmeans_data2) # classify data into 2 pools based on document vectors\n",
    "kmeans_data2['label'] = kmeans3.labels_\n",
    "print(np.unique(kmeans_data2['label']))\n",
    "kmeans_data2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-summer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets labeled 0:\n",
    "zeros = [11,17,25,87,100,114,144,152]\n",
    "for zero in zeros:\n",
    "    print(model_data.tweet[zero], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets labeled 1:\n",
    "ones = [102,134]\n",
    "for one in ones:\n",
    "    print(model_data.tweet[one], '\\n',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-person",
   "metadata": {},
   "source": [
    "### Comparison of Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame()\n",
    "comparison['kmeans1_label'] = kmeans_data0['label']\n",
    "comparison['kmeans2_label'] = kmeans_data1['label']\n",
    "comparison['kmeans3_label'] = kmeans_data2['label']\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(comparison['kmeans1_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(comparison['kmeans2_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(comparison['kmeans3_label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
